# Human Limitations
Cognitive augmentation begins from an acceptance of limitation. Attention, memory, and reasoning are finite resources (Simon, 1957), but this does not represent a flaw to be corrected; rather, these constraints function as design parameters for how to approach the construction of external supports to extend human cognition. Rather than asking how humans can think more like machines, augmentation asks how we can design ecologies of cognition (Hutchins, 1995) where human minds, tools, and representations (internal and external) can cooperate to overcome [[Descriptive Decision-Making Models#Bounded Rationality|Bounded Rationality]]. 

Human cognition evolved for local, low-dimensional, and socially-embedded contexts (Gigerenzer & Todd, 1999). Modern decision environments, which involve algorithmic systems, data-rich uncertainty, and global interdependence, exceed what our intuitive [[Heuristics]] and [[Schema Theory|Schema]] can process without external assistance. As a result, a mismatch between evolved capacities and technological complexity emerges (Norman, 1993). Cognitive augmentation translates complexity into cognitively-traceable forms, through [[Visualization for Cognitive Augmentation Hub|Visualization]], modelling, structured reasoning prompts, or general feedback systems that make internal thought external. These representations act as mediators between human cognition and the complex, abstract domains we must navigate. 

Traditional decision cycles move from perception -> evaluation -> action. Augmented decision-making introduces the concept of meta-cognitive layers: externalization, reflection, and feedback (Kirsch, 2010). Writing, simulation, and [[Visualization for Cognitive Augmentation Hub|Visualization]] that make internal reasoning external, allow for recursive improvements and enhancements. This recursion is a core element of cognitive augmentation - the capacity to think about thinking, or the practice of "metacognition" (Dunlosky & Metcalfe, 2008). It transforms decision-making from a bounded psychological process to an evolving interaction between human and environment. 
# Normative Models as Augmentation Benchmarks
[[Normative Decision-Making Models|Normative Models]] describe optimal decision outcomes under ideal circumstances. In cognitive augmentation, these models are not instructions for humans - they are reference points against which tool-supported reasoning can be evaluated. As an example, consider Influence Diagrams (Koller & Milch, 2003). Influence Diagrams generalize decision trees and use the principle of maximizing [[Normative Decision-Making Models#Expected Utility Theory|Expected Utility]] through [[Normative Decision-Making Models#Bayesian Decision Theory|Bayesian networks]]. The [[Normative Decision-Making Models|Normative Model]] defines which action a rational agent should take via known probabilities and utilities, creating a benchmark. The system supports humans by making the structure explicit (states -> acts -> outcomes -> utilities), and helps them to compare alternatives with uncertainty (Koller & Milch, 2003). This represents a scaffold for human decision-making, where the person is still involved, but is also able to see the "ideal" calculation. 

Another example of augmentation via [[Normative Decision-Making Models]] comes from decision analysis literature: the use of multi-attribute utility/value models which incorporate normative assumptions about rational trade-offs and risk preferences, in organizational or engineering decisions. From Keeney and colleagues' (2007) *Practical Value Models*, they provide a description of how value modelling is used in decision support for high-stakes decisions. Decision-support systems embed [[Normative Decision-Making Models]] as formal value functions, with weights and utilities, and use them to compute "optimal" trade-offs across attributes. Once again, the Normative framework becomes a design scaffold, showcasing how a system may compute what a decision should look like, with which the human user can contrast their findings with. The biggest challenge of augmentation with Normative frameworks is found in the translation from formal models into usable interfaces. 
# Descriptive Models for Human-Centered Design
[[Descriptive Decision-Making Models]] reveal that humans rely on [[Heuristics]], affective cues, and contextual framing when they make judgements. These tendencies can be biased, but are typically functional for real-world cognition. In cognitive augmentation design, the goal is to align tools with human cognitive architecture, as opposed to aiming to eliminate it. Effective human-centered augmentation emphasizes *fit to cognition* (Vessey, 1991) rather than correction of cognition. 

As an example, consider Clinical Decision Support Systems (CDSS). Physicians often rely on heuristics and pattern recognition when under time pressure. A [[Normative Decision-Making Models|normative decision aid]] would impose rigid, probabilistic reasoning, and ignore the expert's contextual intuition. Human-centered CDSS design uses descriptive insights to augment an expert's natural process. Cho and colleagues (2022) examined how heuristic evaluation can inform the usability of CDSS. They found that design elements should mirror clinician's cognitive workflows, supporting recognition and recall rather than forcing abstract rule-following. Augmentation occurs by:

- Providing adaptive, context-relevant prompts instead of static rules.
- Counteracting [[Limitations and Biases of Mental Models|Availability Bias]] through base-rate data visualization.
- Enabling explainable AI feedback to support metacognitive reasoning.

In this fashion, CDSS tools can augment an expert's abilities without superseding their intuition and abilities, working together as a team rather than providing a rigid ruleset. 
# Heuristics and Augmentation
[[Heuristics]] are often seen as simply sources of bias, mental shortcuts that lead only to judgement errors (Tversky & Kahneman, 1974). However, through the lens of cognitive augmentation, heuristics can be seen as adaptive mechanisms which allow humans to make efficient decisions when presented with uncertainty and limited time. Augmentation can seek to extend the scope and accuracy of heuristics, rather than to outright replace them.

1) **Heuristic Amplification**: utilize augmentation to extend the range and precision of heuristics, rather than replace them (based on Klein, 1998). 
2) **Heuristic Reflection**: provide a metacognitive feedback that makes users aware of their reliance on particular heuristics. As an example, a medical diagnostic support tool that flags when pattern recognition overrides contradicted evidence (Croskerry, 2009).
3) **Heuristic Scaffolding**: combine human intuition with algorithm correction loops. An example of this could be a navigation system that aligns with a driver's spatial intuition while dynamically recalibrating routes to prevent [[Heuristics#Anchoring & Adjustment Heuristic|anchoring]] on familiar paths, if those paths are deemed suboptimal. 
## References
- Cho, H., Keenan, G., Madandola, O. O., Dos Santos, F. C., Macieira, T. G. R., Bjarnadóttir, R. I., Priola, K. J. B., & Dunn Lopez, K. (2022). Assessing the usability of a clinical decision support system: Heuristic evaluation. *JMIR Human Factors, 9*(2), e31758. https://doi.org/10.2196/31758  
- Croskerry, P. (2009). *A universal model of diagnostic reasoning.* *Academic Medicine, 84*(8), 1022–1028. https://doi.org/10.1097/ACM.0b013e3181ace703  
- Dunlosky, J., & Metcalfe, J. (2008). _Metacognition_. Sage Publications.
- Gigerenzer, G., & Todd, P. M. (1999). *Simple Heuristics That Make Us Smart.* Oxford University Press.  
- Hutchins, E. (1995). *Cognition in the Wild.* MIT Press.
- Kirsh, D. (2010). *Thinking with external representations.* *AI & Society, 25*(4), 441–454.
- Klein, G. (1998). _Sources of power: How people make decisions._ MIT Press.
- Koller, D., &  Milch, B. (2003). _Multi‑agent influence diagrams for representing and solving games._ _Games and Economic Behavior, 45_(1), 181–221. [https://doi.org/10.1016/S0899-8256(02)00544-4]
- Norman, D. A. (1993). *Things That Make Us Smart: Defending Human Attributes in the Age of the Machine.* Addison-Wesley.
- Keeney, Ralph L., and Detlof von Winterfeldt. “Practical Value Models.” Chapter. In _Advances in Decision Analysis: From Foundations to Applications_, edited by Ward Edwards, Ralph F. Miles Jr., and Detlof von Winterfeldt, 232–52. Cambridge: Cambridge University Press, 2007.
- Simon, H. A. (1957). *Models of Man: Social and Rational.* Wiley.
- Vessey, I. (1991). Cognitive fit: A theory-based analysis of the graphs versus tables literature. _Decision Sciences, 22_(2), 219–240. https://doi.org/10.1111/j.1540-5915.1991.tb00344.x